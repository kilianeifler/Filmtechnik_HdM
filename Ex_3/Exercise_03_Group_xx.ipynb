{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - HDR reconstruction and camera chracterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.random\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from scipy import fftpack\n",
    "from scipy import optimize\n",
    "import scipy.io\n",
    "import skimage\n",
    "import imageio\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# for 16 bit PNG support in imageio if needed, imageio.plugins.freeimage.download() might need to be run once per system (or exr/hdr/pgm/ppm support)\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "def applyColorMatrix(values, mat):\n",
    "    tmp = values.reshape(-1, 3)\n",
    "    tmp = np.matmul(tmp, mat)\n",
    "    return np.reshape(tmp, values.shape)\n",
    "\n",
    "def lin2srgb(x):\n",
    "    # Your code here\n",
    "    # Your code here\n",
    "    # Your code here\n",
    "    # Your code here\n",
    "\n",
    "def srgb2lin(x):\n",
    "    # Your code here\n",
    "    # Your code here\n",
    "    # Your code here\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 HDR reconstruction\n",
    "\n",
    "In a camera light enters the lens and illuminates the sensor. This illuminance has a linear relationship to the radiance of the scene. The illuminance has a linear relationship to the integration time, it is proportional to the square of the f-stop and reciprocal to the square of the focal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because cameras have a limited dynamic range multiple exposures with different integration times are needed to measure the full dynamic range of most natural scenes. \n",
    "# Why is it preferred to change the exposure time over changing the f-stop for the different exposures when acquiring HDR Stacks?\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# First load the demosaiced rgb images from the last exercise\n",
    "rgbImg = np.load('../exercise_02/rgb_exposure_stack.npmat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To illustrate how much data we have in memory:\n",
    "print('Image shape:', rgbImg.shape)\n",
    "print('Memory consumption:', np.prod( rgbImg.shape ) * 8 / 1024 / 1024, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now estimate the exposure time for Haus_02.tiff to Haus_07.tiff relative to Haus_01.tiff by finding the factor that minimizes the sum of squared differences between both images and by dividing the relavant pixels of each picture to its neighbouring image and calculating the mean and median of all these fractions.\n",
    "# Hint: Make sure you exclude the noisy and clipped pixels for both images to get the correct result. \n",
    "\n",
    "# Define a range of valid luminances:\n",
    "highCut = 0.9  # Ignore the highest 10% beacuse lots of cameras do weird stuff there\n",
    "lowCut  = 0.01 # Limit to the top 6 stops because it gets noisy below 6 stops under peak white\n",
    "\n",
    "# We will try 3 methods: Least square optimization, median and mean\n",
    "ev_ls     = np.zeros(6)\n",
    "ev_median = np.zeros(6)\n",
    "ev_mean   = np.zeros(6)\n",
    "\n",
    "for i in range(6):\n",
    "    # This is the base image (Use only a small fraction of pixels for debugging)\n",
    "    img0 = rgbImg[:,:,:, i]\n",
    "\n",
    "    # This is the next image (Use only a small fraction of pixels for debugging)\n",
    "    img1 = rgbImg[:,:,:, i+1]\n",
    "\n",
    "    # Build a mast that only includes those pixel that are within LowCut and HighCut for both images\n",
    "    pix_mask = # Your code here\n",
    "\n",
    "    # Show what is used from the current images\n",
    "    plt.style.use('dark_background')\n",
    "    fig,a =  plt.subplots(1, 4)\n",
    "    a[0].imshow( ( np.log2( np.fmax( 2**-10, np.fmin( 1.0, img0 ) ) ) + 10 ) / 10, cmap='gray', vmin=0, vmax=1)\n",
    "    a[0].set_title(\"img0\")\n",
    "    a[1].imshow( pix_mask[:,:,0] * 1.0, cmap='gray', vmin=0, vmax=1)\n",
    "    a[1].set_title(\"Valid r Pixels\")\n",
    "    a[2].imshow( pix_mask[:,:,1] * 1.0, cmap='gray', vmin=0, vmax=1)\n",
    "    a[2].set_title(\"Valid g Pixels\")\n",
    "    a[3].imshow( pix_mask[:,:,2] * 1.0, cmap='gray', vmin=0, vmax=1)\n",
    "    a[3].set_title(\"Valid b Pixels\")\n",
    "\n",
    "    # Define a function for minimization ( least squares needs the squared difference as cost function ):\n",
    "    def minFun(x):\n",
    "        return # Your code here\n",
    "    \n",
    "    # Use optimization: Unfortunately \"optimize.fmin\" is quite slow\n",
    "    ev_ls[i]     = optimize.fmin( func=minFun, x0=[1.0], disp=False)[0]    \n",
    "\n",
    "    # Calculate the median of the valid pixels of img0 divided by img1\n",
    "    ev_median[i] = # Your code here\n",
    "\n",
    "    # Calculate the mean of the valid pixels of img0 divided by img1\n",
    "    ev_mean[i]   = # Your code here\n",
    "\n",
    "print('LeastSquare', ev_ls)\n",
    "print('Medians    ', ev_median)\n",
    "print('Means      ', ev_mean)\n",
    "\n",
    "# Which method do you prefer in terms of speed of execution and precision of the result?\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's reconstruct the original radiance of the scene from the image stack. \n",
    "# Weight the luminance values by a weight-function that gives more credible values a higher value and completely discards noisy or clipped values. \n",
    "\n",
    "# First write a hat funtion for weighting\n",
    "def hatFun(x, l, m, h):\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here\n",
    "     # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the hat function (also for values out of range 0...1)\n",
    "fig, ax = plt.subplots( figsize=(15,7) )\n",
    "\n",
    "x = np.linspace(-0.1, 1.1, 1000)\n",
    "plt.plot( x, hatFun(x, 0.1, 0.4, 0.78 ) );\n",
    "plt.grid( linestyle=':' )\n",
    "\n",
    "# Reference how it schould look like:\n",
    "newax = fig.add_axes( [0.59, 0.56, 0.3, 0.3], anchor='NE', zorder=1 )\n",
    "newax.imshow( plt.imread('additional/hat_fun_plot.png') / 0.8 + 0.1, interpolation = 'catrom' );\n",
    "newax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Weights for each pixel in each color and stack image. \n",
    "# Make sure to capture all pixels even if they are not within the hat function in any picture\n",
    "hdrWeights = # Your code here\n",
    "# Your code here\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute exposure correction from the realtive exposure differences 'ev_median'\n",
    "evCorrection = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstuct an HDR image from the exposure stack by combining the image according to the weights.\n",
    "# Don’t forget to normalize the result for each color channel and pixel by the sum of your weights to get the correct result.\n",
    "hdrImg = # Your code here\n",
    "hdrImg[np.isnan(hdrImg)] = 0.0  # a few pixels might be nan. If no correct weight was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify the shadows. Your reconstruction should be as clean as in the lowlight preserving exposure\n",
    "plt.style.use('dark_background')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lin2srgb(rgbImg[1000:2500,600:1500,:,0] / np.prod(ev_median)))\n",
    "plt.title('Noisy shadows in the\\nhighlight Preserving Exposure')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.style.use('dark_background')\n",
    "plt.imshow(lin2srgb(rgbImg[1000:2500,600:1500,:,6]))\n",
    "plt.title('Clean shadows in the\\nlowlight preserving exposure')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.style.use('dark_background')\n",
    "plt.imshow(lin2srgb(hdrImg[1000:2500,600:1500,:] / np.prod(ev_median)))\n",
    "plt.title('Clean shadows in the\\nHDR reconstruction')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( plt.imread('additional/Source_vs_Reconstruction_1.png'), interpolation = 'catrom' );\n",
    "plt.title('This is how your result should look like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify the highlights. Your reconstruction should be as unclipped as the highlight preserving exposure\n",
    "plt.style.use('dark_background')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lin2srgb(rgbImg[:,:,:,0]))\n",
    "plt.title('Sky looks fine in the\\nhighlight preserving exposure')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(lin2srgb(rgbImg[:,:,:,6] * np.prod( ev_median )))\n",
    "plt.title('Sky is completely clipped\\nin the lowlight preserving exposure')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(lin2srgb(hdrImg))\n",
    "plt.title('All details there in the\\nHDR reconstruction')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( plt.imread('additional/Source_vs_Reconstruction_2.png'), interpolation = 'catrom' );\n",
    "plt.title('This is how your result should look like')\n",
    "# Where do the  color artifacts around the clouds in the sky come from? How could they be avoided?\n",
    "#\n",
    "#\n",
    "#\n",
    "# Bonus points: Shoot your own exposure stack and process it using this script. Make sure you dont touch your camerera between exposures, e.g. use an app to control your camera. Frank (Pool) has Color Checkers for rent. the color checker needs to be fully hoizontal and vertically aligned with the image orientation. You will find out why in the next part of this exercise.\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Camera characterization\n",
    "\n",
    "Looking at the image from exercise 3.1 it can be seen that the colors are not displayed faithfully. We will use three different methods that we learned in the lecture to adjust the colors from the native color space of the 5DMkII to sRGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are ther reference Color Checker Values from Babelcolor\n",
    "cc_ref = np.array([[115, 82, 68], [195, 149, 128], [93, 123, 157], [91, 108, 65], [130, 129, 175], [99, 191, 171], [220, 123, 46], [72, 92, 168], [194, 84, 97], [91, 59, 104], [161, 189, 62], [229, 161, 40], [42, 63, 147], [72, 149, 72], [175, 50, 57], [238, 200, 22], [188, 84, 150], [0, 137, 166], [245, 245, 240], [201, 202, 201], [161, 162, 162], [120, 121, 121], [83, 85, 85], [50, 50, 51]], dtype=np.float)\n",
    "\n",
    "# Convert these reference values from 8 bit sRGB to scene linear\n",
    "cc_ref = srgb2lin(cc_ref / 255.0)  #R cc_ref = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position values/vsize/hsize multiplied by two to reflect position difference from MATLAB (half debayer) to bilinear debayer\n",
    "\n",
    "def getCheckerValuesFromHausImage(img, no_checker=False):\n",
    "    \"\"\"\n",
    "    Helper function to extract the color checker patches median values for all 24 color patches of the MacBeth chart\n",
    "    \"\"\"\n",
    "    crop_values = np.concatenate(( np.ones((1,6)) * 2053,\n",
    "                                   np.ones((1,6)) * 2090,\n",
    "                                   np.ones((1,6)) * 2127,\n",
    "                                   np.ones((1,6)) * 2163), axis=1) \n",
    "    crop_values = np.concatenate(( crop_values,\n",
    "                                   np.array([1046, 1081, 1117, 1154, 1190, 1227,   1046, 1081, 1118, 1155, 1191, 1228, 1046, 1081, 1118, 1155, 1191, 1228,   1046, 1081, 1118, 1155, 1191, 1228])[:,None].T  - 1.5), axis=0) - 5\n",
    "    crop_values = np.uint32(crop_values)\n",
    "\n",
    "    vsize = 26\n",
    "    hsize = 26 \n",
    "\n",
    "    cc_values = np.zeros((24, 3))\n",
    "\n",
    "    select = np.zeros((img.shape[0], img.shape[1]), dtype=int)\n",
    "\n",
    "    for i in range(24):\n",
    "        # using nanmedian in case we somehow have an invalid pixel value (nanmedian ignores nan-values)\n",
    "        tmp = np.nanmedian(img[crop_values[0, i]:crop_values[0, i]+hsize, crop_values[1, i]:crop_values[1, i]+vsize, :], axis=0)\n",
    "        cc_values[i, :] = np.nanmedian(tmp, axis=0)\n",
    "        select[crop_values[0, i]:crop_values[0, i]+hsize, crop_values[1, i]:crop_values[1, i]+vsize] = 1\n",
    "\n",
    "    if not no_checker:\n",
    "        errosion_structure = ndimage.generate_binary_structure(2, 1)\n",
    "        select = select - ndimage.grey_erosion(select, size=(3,3), footprint=errosion_structure)\n",
    "        \n",
    "        tmp_img = img / np.max(img) + select[:,:,None]\n",
    "        tmp_img = tmp_img[2030:2210, 1010:1290,:]\n",
    "        plt.imshow(lin2srgb(tmp_img / np.max(tmp_img)))\n",
    "        plt.show()\n",
    "\n",
    "    return cc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the green cast from the image one could scale all three color channels so that the white patch of the color checker ( 19th Patch ) results in the target values from Babelcolor converted to linear. Use the supplied function Exercise_04_getCheckerValuesFromHausImage.m to extract the color checker values from your image. \n",
    "\n",
    "# Exttract the actual values from the color checker\n",
    "cc_vals = getCheckerValuesFromHausImage(hdrImg)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( plt.imread('additional/cc_select.png'), interpolation = 'catrom' );\n",
    "plt.title('This is how your result should look like')\n",
    "\n",
    "cc_vals.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Scale the three color channels accordingly and display the image. \n",
    "\n",
    "# * Hint 1: It is common to have diffuse white at 1.0 in linear light images. One stop below white is at 0.5, two stops below white at 0.25 and typical Caucasian skin tones at 0.18. Specular highlights and light sources can be way above 1.0.\n",
    "\n",
    "imgCCbyScaling = np.zeros(hdrImg.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    imgCCbyScaling[:,:,i] = hdrImg[:,:,i] # Your code here\n",
    "\n",
    "plt.imshow( lin2srgb( imgCCbyScaling ) )\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( plt.imread('additional/Result_RGB_Scaling.png'), interpolation = 'catrom' );\n",
    "plt.title('This is how your result should look like')\n",
    "\n",
    "# You have solved one problem, but which problem persists?\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Direct inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second approach to camera characterization that you learned in the lecture is the direct inverse. Calculate the matrix that results in the correct color values for the red, green and blue patch.\n",
    "\n",
    "# Get the source values again\n",
    "cc_vals = getCheckerValuesFromHausImage(hdrImg, no_checker=True)\n",
    "\n",
    "# Calculate the direct inverse. Yes, this is only one line, but it may take an hour of remebering matrix algebra\n",
    "direct_inverse = # Your code here. Direct inverse shoud be a 3*3 matrix.\n",
    "\n",
    "img_di = applyColorMatrix( hdrImg, direct_inverse )\n",
    "\n",
    "plt.imshow( lin2srgb( img_di ) )\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( plt.imread('additional/Result_Direct_Inverse.png'), interpolation = 'catrom' );\n",
    "plt.title('This is how your result should look like')\n",
    "\n",
    "# What do you like about this method? What could be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the pseudoinverse using all 24 patches in linear RGB domain. If you are unsure how to find the matrix that minimizes the squared error for all 24 patches search for ‘linear least squares’ or ‘normal equation’. You may also want to watch this graphical explanation of the least squares pseudo inverse: https://www.youtube.com/watch?v=Z0wELiinNVQ&nohtml5=False. (0:00 – 8:40)\n",
    "\n",
    "# Apply the matrix you calculated to your reconstructed image. \n",
    "\n",
    "cc_vals = getCheckerValuesFromHausImage(hdrImg, no_checker=True)\n",
    "\n",
    "pseudo_inverse = # Your code here. Direct inverse shoud be a 3*3 matrix.\n",
    "img_pi = applyColorMatrix( hdrImg, pseudo_inverse )\n",
    "\n",
    "plt.imshow(lin2srgb(img_pi))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( plt.imread('additional/Result_Pseudo_Inverse.png'), interpolation = 'catrom' );\n",
    "plt.title('This is how your result should look like')\n",
    "\n",
    "# Compare the colors to both methods before. Which method do you prefer and why?\n",
    "#\n",
    "#\n",
    "\n",
    "# In which domain would an actual color scientist minimize the squared error for the pseudo inverse matrix? What Python function would you use for this?\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use!\n",
    "np.save('hdr_image_pinv', img_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}